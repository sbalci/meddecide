---
title: "Inter-Rater Agreement and Reliability Analysis in Pathology"
author: "ClinicoPath Development Team"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)
```

# Introduction

Inter-rater agreement and reliability analysis are fundamental to pathology practice, quality assurance, and research. This comprehensive guide covers agreement analysis for pathologists, from basic Cohen's kappa to advanced methods for complex study designs.

**Learning Objectives:**

- Understand types of agreement measures and their appropriate use
- Master interpretation of kappa statistics in pathological contexts
- Learn to design and power agreement studies
- Apply agreement analysis to different pathological scenarios
- Implement quality assurance programs using agreement metrics

## Clinical Applications in Pathology

Agreement analysis is essential for:

1. **Diagnostic Consistency**: Evaluating inter-pathologist agreement on diagnoses
2. **Biomarker Scoring**: Assessing concordance in IHC, FISH, and molecular markers
3. **Grading Systems**: Validating tumor grading and staging protocols
4. **Digital Pathology**: Comparing traditional vs. digital diagnosis
5. **Training Programs**: Monitoring resident and fellow progress
6. **Quality Assurance**: Establishing institutional benchmarks

```{r load-packages}

library(dplyr)
library(ggplot2)
library(knitr)
library(irr)
```

# Dataset Overview

## Comprehensive Agreement Datasets

The package includes multiple datasets for different agreement scenarios:

### Breast Cancer Agreement Study
```{r load-breast-data}
# Load breast cancer pathologist agreement data
data(breast_agreement_data)

# Overview of the study
str(breast_agreement_data)
cat("Study includes:", nrow(breast_agreement_data), "cases from", 
    length(unique(breast_agreement_data$Institution)), "institutions\n")

# Preview the data structure
head(breast_agreement_data)
```

This dataset represents a multi-institutional study of breast cancer diagnosis agreement with:
- **3 pathologists** with different experience levels
- **200 cases** including various breast lesions  
- **4 diagnostic categories**: Benign, Atypical, DCIS, Invasive Carcinoma
- **Institutional variability** across academic and community settings

### Melanoma Agreement Study
```{r load-melanoma-data}
# Load melanoma agreement data for histologic features
data(melanoma_agreement_data)

# Study characteristics
table(melanoma_agreement_data$Histologic_Subtype)
table(melanoma_agreement_data$Breslow_Category)
```

# Types of Agreement Analysis

## 1. Binary Classifications (Cohen's Kappa)

### Basic Two-Rater Agreement
```{r cohens-kappa-basic}
# Evaluate agreement between two experienced pathologists
agreement_result <- agreement(
  data = breast_agreement_data,
  rater1_var = "Pathologist_1_Diagnosis",
  rater2_var = "Pathologist_2_Diagnosis",
  agreement_type = "kappa",
  conf_level = 0.95
)

# View results
print(agreement_result)

# Interpretation
if (agreement_result$kappa > 0.81) {
  cat("Agreement is ALMOST PERFECT (κ >0.81)")
} else if (agreement_result$kappa > 0.61) {
  cat("Agreement is SUBSTANTIAL (κ 0.61-0.80)")
} else if (agreement_result$kappa > 0.41) {
  cat("Agreement is MODERATE (κ 0.41-0.60)")
} else {
  cat("Agreement needs improvement (κ ≤0.40)")
}
```

### Weighted Kappa for Ordinal Categories
```{r weighted-kappa}
# For ordinal variables like tumor grades
# Convert to ordered factors if needed
breast_agreement_data$Grade_1 <- factor(breast_agreement_data$Grade_Pathologist_1, 
                                        levels = 1:3, ordered = TRUE)
breast_agreement_data$Grade_2 <- factor(breast_agreement_data$Grade_Pathologist_2, 
                                        levels = 1:3, ordered = TRUE)

# Weighted kappa accounts for degree of disagreement
weighted_agreement <- agreement(
  data = breast_agreement_data,
  rater1_var = "Grade_1",
  rater2_var = "Grade_2", 
  agreement_type = "weighted_kappa",
  weights = "quadratic",  # Penalizes larger disagreements more
  conf_level = 0.95
)

print(weighted_agreement)
```

## 2. Multiple Raters (Fleiss' Kappa)

### Three-Pathologist Study Design
```{r fleiss-kappa}
# Reshape data for multiple raters
library(dplyr)

# Create matrix format required for Fleiss' kappa
multi_rater_data <- breast_agreement_data %>%
  select(Case_ID, Pathologist_1_Diagnosis, Pathologist_2_Diagnosis, Pathologist_3_Diagnosis)

# Calculate Fleiss' kappa
fleiss_result <- agreement(
  data = multi_rater_data,
  rater_vars = c("Pathologist_1_Diagnosis", "Pathologist_2_Diagnosis", "Pathologist_3_Diagnosis"),
  agreement_type = "fleiss",
  conf_level = 0.95
)

print(fleiss_result)
```

### Category-Specific Agreement
```{r category-specific}
# Examine agreement for each diagnostic category
category_agreement <- agreement(
  data = multi_rater_data,
  rater_vars = c("Pathologist_1_Diagnosis", "Pathologist_2_Diagnosis", "Pathologist_3_Diagnosis"),
  agreement_type = "fleiss",
  category_specific = TRUE
)

# Plot category-specific kappa values
plot(category_agreement, type = "category")
```

## 3. Continuous Measurements (ICC)

### Intraclass Correlation Coefficient
```{r icc-analysis}
# For continuous measurements like Ki-67 percentages
icc_result <- icccoeff(
  data = breast_agreement_data,
  rater_vars = c("Ki67_Pathologist_1", "Ki67_Pathologist_2", "Ki67_Pathologist_3"),
  icc_type = "ICC(2,k)",  # Two-way random effects, average measures
  conf_level = 0.95
)

print(icc_result)

# ICC interpretation:
# < 0.50: Poor reliability
# 0.50-0.75: Moderate reliability  
# 0.75-0.90: Good reliability
# > 0.90: Excellent reliability
```

### Bland-Altman Analysis
```{r bland-altman}
# For method comparison studies
bland_altman_plot <- agreement(
  data = breast_agreement_data,
  method1 = "Ki67_Pathologist_1",
  method2 = "Ki67_Pathologist_2",
  analysis_type = "bland_altman",
  plot_type = "difference",
  limits_of_agreement = TRUE
)

print(bland_altman_plot)
```

# Factors Affecting Agreement

## Experience Level Analysis
```{r experience-effect}
# Compare agreement by pathologist experience
experience_comparison <- breast_agreement_data %>%
  group_by(Pathologist_Experience) %>%
  summarise(
    agreement_with_consensus = agreement(
      rater1_var = "Pathologist_Diagnosis",
      rater2_var = "Consensus_Diagnosis",
      agreement_type = "kappa"
    )$kappa,
    .groups = "drop"
  )

print(experience_comparison)

# Visualize experience effect
ggplot(experience_comparison, aes(x = Pathologist_Experience, y = agreement_with_consensus)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Agreement with Consensus by Experience Level",
       x = "Pathologist Experience", 
       y = "Kappa Coefficient") +
  theme_minimal()
```

## Case Difficulty Analysis
```{r difficulty-effect}
# Examine agreement by case difficulty
difficulty_analysis <- breast_agreement_data %>%
  mutate(
    case_difficulty = case_when(
      Consensus_Confidence >= 4 ~ "Easy",
      Consensus_Confidence == 3 ~ "Moderate", 
      Consensus_Confidence <= 2 ~ "Difficult"
    )
  ) %>%
  group_by(case_difficulty) %>%
  summarise(
    kappa = agreement(
      rater1_var = "Pathologist_1_Diagnosis",
      rater2_var = "Pathologist_2_Diagnosis", 
      agreement_type = "kappa"
    )$kappa,
    n_cases = n(),
    .groups = "drop"
  )

print(difficulty_analysis)
```

# Study Design and Sample Size Planning

## Power Analysis for Agreement Studies

### Binary Outcomes
```{r power-binary}
# Calculate sample size for detecting moderate vs substantial agreement
power_analysis <- kappaSizePower(
  outcome = "2",           # Binary outcome
  kappa0 = 0.4,           # Null hypothesis (moderate agreement)  
  kappa1 = 0.6,           # Alternative hypothesis (substantial agreement)
  props = "0.3, 0.7",     # Expected proportions (30% positive, 70% negative)
  raters = "2",           # Two raters
  alpha = 0.05,           # Type I error
  power = 0.80            # Statistical power
)

print(power_analysis)
```

### Multiple Categories
```{r power-multicategory}
# Sample size for 4-category diagnostic classification
power_multicat <- kappaSizePower(
  outcome = "4",
  kappa0 = 0.5,
  kappa1 = 0.7, 
  props = "0.25, 0.25, 0.25, 0.25",  # Equal proportions
  raters = "2",
  alpha = 0.05,
  power = 0.80
)

print(power_multicat)
```

## Precision-Based Sample Size

### Confidence Interval Approach
```{r precision-sample-size}
# Sample size for desired precision (CI width)
precision_analysis <- kappaSizeCI(
  outcome = "2",
  kappa0 = 0.6,          # Expected kappa
  conf_level = 0.95,     # Confidence level
  width = 0.2,           # Desired CI width (±0.1 around kappa)
  props = "0.4, 0.6",
  raters = "2"
)

print(precision_analysis)
```

### Fixed Sample Size Analysis
```{r fixed-n-analysis}
# What precision can we achieve with available sample?
fixed_n_result <- kappaSizeFixedN(
  outcome = "2",
  kappa0 = 0.6,
  props = "0.3, 0.7", 
  raters = "2",
  conf_level = 0.95,
  n = 150               # Available sample size
)

print(fixed_n_result)
```

# Advanced Applications

## Conditional Kappa Analysis
```{r conditional-kappa}
# Agreement conditional on true positive/negative status
conditional_agreement <- agreement(
  data = breast_agreement_data,
  rater1_var = "Pathologist_1_Diagnosis",
  rater2_var = "Pathologist_2_Diagnosis",
  truth_var = "Gold_Standard",
  analysis_type = "conditional_kappa"
)

print(conditional_agreement)
```

## Multi-Level Agreement (Hierarchical)
```{r multilevel-agreement}
# Account for clustering by institution
library(lme4)

multilevel_model <- agreement(
  data = breast_agreement_data,
  rater1_var = "Pathologist_1_Diagnosis",
  rater2_var = "Pathologist_2_Diagnosis", 
  cluster_var = "Institution",
  analysis_type = "multilevel_kappa"
)

print(multilevel_model)
```

## Longitudinal Agreement (Test-Retest)
```{r test-retest}
# Agreement across time points
test_retest <- agreement(
  data = breast_agreement_data,
  time1_var = "Initial_Diagnosis",
  time2_var = "Repeat_Diagnosis",
  subject_id = "Case_ID",
  analysis_type = "test_retest"
)

print(test_retest)
```

# Quality Assurance Applications

## Establishing QA Benchmarks
```{r qa-benchmarks}
# Set institutional benchmarks
qa_standards <- list(
  diagnostic_agreement = list(
    minimum_kappa = 0.6,
    target_kappa = 0.8,
    benchmark_cases = 100
  ),
  biomarker_scoring = list(
    minimum_icc = 0.75,
    target_icc = 0.90,
    benchmark_cases = 50
  )
)

# Monitor ongoing performance
current_performance <- agreement(
  data = breast_agreement_data,
  rater1_var = "Pathologist_1_Diagnosis",
  rater2_var = "Consensus_Diagnosis",
  agreement_type = "kappa"
)

# Check against benchmark
if (current_performance$kappa >= qa_standards$diagnostic_agreement$target_kappa) {
  cat("✓ Performance EXCEEDS target benchmark")
} else if (current_performance$kappa >= qa_standards$diagnostic_agreement$minimum_kappa) {
  cat("⚠ Performance meets minimum but below target")
} else {
  cat("✗ Performance BELOW minimum standard - intervention needed")
}
```

## Training Assessment
```{r training-assessment}
# Monitor trainee progress over time
trainee_progress <- breast_agreement_data %>%
  filter(Pathologist_Level == "Resident") %>%
  group_by(Training_Month) %>%
  summarise(
    agreement_with_attending = agreement(
      rater1_var = "Resident_Diagnosis",
      rater2_var = "Attending_Diagnosis", 
      agreement_type = "kappa"
    )$kappa,
    .groups = "drop"
  )

# Plot learning curve
ggplot(trainee_progress, aes(x = Training_Month, y = agreement_with_attending)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0.6, linetype = "dashed", color = "red", 
             alpha = 0.7) +
  annotate("text", x = max(trainee_progress$Training_Month) * 0.8, y = 0.65, 
           label = "Minimum Standard", color = "red") +
  labs(title = "Resident Training Progress: Agreement with Attending",
       x = "Training Month", 
       y = "Kappa Coefficient") +
  theme_minimal()
```

# Interpretation Guidelines

## Clinical Significance vs Statistical Significance
```{r clinical-vs-statistical}
# Large sample may show statistical significance for clinically unimportant differences
large_study_result <- agreement(
  data = large_agreement_dataset,  # Hypothetical large dataset
  rater1_var = "Rater1", 
  rater2_var = "Rater2",
  agreement_type = "kappa"
)

# Consider both p-value AND effect size
cat("Statistical significance: p =", large_study_result$p_value, "\n")
cat("Clinical significance: κ =", large_study_result$kappa, "\n")

if (large_study_result$p_value < 0.05 & large_study_result$kappa < 0.4) {
  cat("Statistically significant but clinically inadequate agreement")
}
```

## Reporting Standards
```{r reporting-template}
# Template for reporting agreement results
agreement_report <- function(result) {
  cat("Agreement Analysis Report\n")
  cat("========================\n\n")
  cat("Sample size:", result$n, "cases\n")
  cat("Kappa coefficient:", round(result$kappa, 3), 
      "(95% CI:", round(result$ci_lower, 3), "-", round(result$ci_upper, 3), ")\n")
  cat("P-value:", format.pval(result$p_value), "\n")
  cat("Interpretation:", result$interpretation, "\n")
  cat("Recommendation:", result$recommendation, "\n")
}

# Example usage
main_result <- agreement(
  data = breast_agreement_data,
  rater1_var = "Pathologist_1_Diagnosis",
  rater2_var = "Pathologist_2_Diagnosis",
  agreement_type = "kappa"
)

agreement_report(main_result)
```

# Best Practices and Recommendations

## Study Design Considerations

1. **Sample Size Planning**
   - Use power analysis for hypothesis testing
   - Use precision analysis for descriptive studies
   - Account for expected agreement levels and prevalence

2. **Rater Selection** 
   - Include raters representative of intended users
   - Balance experience levels appropriately
   - Ensure adequate training before study

3. **Case Selection**
   - Include full spectrum of difficulty
   - Ensure adequate representation of all categories
   - Consider enrichment for rare categories

## Statistical Considerations

1. **Choose Appropriate Measure**
   - Cohen's kappa: Two raters, nominal/ordinal data
   - Weighted kappa: Ordinal data with meaningful ordering
   - Fleiss' kappa: Multiple raters, nominal data
   - ICC: Continuous measurements

2. **Account for Prevalence Effects**
   - Kappa can be low even with high percentage agreement if prevalence is extreme
   - Report both kappa and percentage agreement
   - Consider prevalence-adjusted kappa when appropriate

3. **Multiple Comparisons**
   - Adjust p-values when testing multiple agreements
   - Focus on clinically important comparisons
   - Report confidence intervals rather than just p-values

## Regulatory Considerations

1. **FDA Guidance Compliance**
   - Document predefined acceptable agreement levels
   - Include appropriate statistical analysis plan
   - Consider regulatory precedents for similar devices/tests

2. **Quality Assurance Integration**
   - Establish ongoing monitoring procedures
   - Define trigger levels for intervention
   - Document corrective action procedures

---

This comprehensive guide provides pathologists and clinical researchers with the tools and knowledge needed to conduct rigorous agreement studies, from initial planning through final interpretation and quality assurance implementation.
