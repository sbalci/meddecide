% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agreement.h.R
\name{agreement}
\alias{agreement}
\title{Interrater Reliability}
\usage{
agreement(
  data,
  vars,
  baConfidenceLevel = 0.95,
  proportionalBias = FALSE,
  blandAltmanPlot = FALSE,
  sft = FALSE,
  wght = "unweighted",
  exct = FALSE,
  kripp = FALSE,
  krippMethod = "nominal",
  bootstrap = FALSE,
  showSummary = FALSE,
  showAbout = FALSE
)
}
\arguments{
\item{data}{The data as a data frame. The data should be in long format,
where each row is a unique observation.}

\item{vars}{A string naming the variable from \code{data} that contains the
diagnosis given by the observer, variable can be categorical or ordinal.}

\item{baConfidenceLevel}{Confidence level for Bland-Altman limits of
agreement (LoA). Typically 0.95 for 95\\% confidence intervals.}

\item{proportionalBias}{Test whether the difference between raters changes
systematically with the magnitude of measurement (proportional bias). Uses
linear regression of difference vs. mean.}

\item{blandAltmanPlot}{Generate Bland-Altman plot for continuous agreement
analysis. Displays mean difference and limits of agreement between the
first two raters. Only applicable when raters provide continuous
measurements (e.g., tumor size in mm).}

\item{sft}{Display frequency tables showing the distribution of ratings for
each rater. Useful for understanding rating patterns and identifying
potential biases.}

\item{wght}{For ordinal variables (e.g., tumor grade G1/G2/G3), weighted
kappa accounts for degree of disagreement. Linear weights: Adjacent
disagreements (G1 vs G2) receive partial credit. Squared weights: Larger
disagreements (G1 vs G3) are penalized more heavily. Use 'Unweighted' for
nominal categories with no inherent order.}

\item{exct}{Use exact p-value calculation instead of normal approximation.
Recommended for small sample sizes (< 30 cases) with 3 or more raters.
Note: Not applicable for 2-rater analysis (use Cohen's kappa).}

\item{kripp}{Alternative reliability measure that handles missing data and
supports various data types. Useful when raters didn't rate all cases or
when comparing different measurement levels.}

\item{krippMethod}{Specifies the measurement level for Krippendorff's alpha
calculation.}

\item{bootstrap}{Calculate bootstrap confidence intervals for
Krippendorff's alpha.}

\item{showSummary}{Display a natural-language interpretation of results
with color-coded agreement levels and clinical guidance. Recommended for
reports and presentations.}

\item{showAbout}{Display an explanatory panel describing what this analysis
does, when to use it, and how to interpret results.}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$welcome} \tab \tab \tab \tab \tab a html \cr
\code{results$irrtable} \tab \tab \tab \tab \tab a table \cr
\code{results$contingencyTable} \tab \tab \tab \tab \tab a table \cr
\code{results$ratingCombinationsTable} \tab \tab \tab \tab \tab a table \cr
\code{results$blandAltman} \tab \tab \tab \tab \tab an image \cr
\code{results$blandAltmanStats} \tab \tab \tab \tab \tab a table \cr
\code{results$krippTable} \tab \tab \tab \tab \tab a table \cr
\code{results$weightedKappaGuide} \tab \tab \tab \tab \tab a html \cr
\code{results$summary} \tab \tab \tab \tab \tab a html \cr
\code{results$about} \tab \tab \tab \tab \tab a html \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$irrtable$asDF}

\code{as.data.frame(results$irrtable)}
}
\description{
Function for Interrater Reliability.
}
\examples{
\donttest{
# example will be added
}
}
